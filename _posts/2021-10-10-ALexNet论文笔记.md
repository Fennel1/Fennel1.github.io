---
layout:     post
title:      ALexNet论文笔记
subtitle:   ImageNet Classification with Deep Convolutional Neural Networks
date:       2021-10-10
author:     fennel
header-img: /my_img/resnest50.jpg
catalog: false
tags:
    - 论文笔记
---


## 论文pdf
- [ImageNet Classification with Deep Convolutional Neural Networks](/paper/ALexNet.pdf)

## 前言
![ImageNet图像识别挑战赛](/my_img/imagenet.png)
ALexNet是2012年 ImageNet 2012 图像识别挑战赛的冠军，并且Top-5错误率到达了15.3%，比第二名低了10.8个百分点。<br>

本来在1998年LeNet提出后，卷积神经网络在计算机视觉和机器学习领域中很有名气。但是因为缺少大量带有标签的数据、硬件计算速度很慢，
所以在上世纪90年代到2012年的大部分时间里，卷积神经网络并不比其他机器学习方法优秀（如支持向量机）。<br>

在2009年，ImageNet数据集发布，并发起ImageNet挑战赛：要求研究人员从100万个样本中训练模型，以区分1000个不同类别的对象。ImageNet数据集由斯坦福教授李飞飞小组的研究人员开发，利用谷歌图像搜索（Google Image Search）对每一类图像进行预筛选，并利用亚马逊众包（Amazon Mechanical Turk）来标注每张图片的相关类别。且随着硬件的不断迭代，计算速度有了很大的提高。随着ALexNet的横空出世，引起了深度学习的热潮。

---

## Tricks

### ReLU 激活函数

![ReLU激活函数](/my_img/relu.png)
![ReLU激活函数公式](https://latex.codecogs.com/png.image?\dpi{110}&space;ReLU(x)&space;=&space;max(0,&space;x))

比起sigmoid、tanh激活函数，ReLU函数有如下的优点：
- ReLU函数计算量小，运算速度快
- ReLU函数是非饱和函数非线性，不易发生梯度消失
- ReLU使部分神经元输出为0，减少参数间的相互依赖关系，从而缓解过拟合现象,加快收敛

#### 函数的饱和性

右饱和：当x趋向于正无穷时，函数的导数趋近于0，此时称为右饱和。<br>
左饱和：当x趋向于负无穷时，函数的导数趋近于0，此时称为左饱和。<br>
饱和函数和非饱和函数：当一个函数既满足右饱和，又满足左饱和，则称为饱和函数，否则称为非饱和函数。<br>


### GPU 并行计算

ALex使用了两个GTX 580 3GB GPU进行并行计算，将网络分布在两个GPU上。我笔记本只有cpu:( <br>
GPU能够直接读取和写入彼此的内存而无需经过cpu内存。所以并行计算方案是每个GPU上有一半的神经元，且在某些层中进行通信。
例如：第三层的神经元从两个GPU的所有神经元上获取输入；而第四层的神经元仅从处于同一GPU的神经元上获取输入。

> This means that, for example, the kernels of layer 3 take input from all kernel maps in layer 2.
However, kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU.

### 重叠池化层

### 数据增强

### Dropout层

## Architecture
