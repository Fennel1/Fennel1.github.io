---
layout:     post
title:      ResNet论文笔记
subtitle:   Deep Residual Learning for Image Recognition
date:       2021-10-22
author:     fennel
header-img: /my_img/resnest50.jpg
catalog: true
tags:
    - 论文笔记
---

## 论文pdf

- [Deep Residual Learning for Image Recognition](/paper/resnet.pdf)

## 前言

![resnetf1](/my_img/resnetf1.png)

**Purpose：** 更深层的神经网络往往更加难以训练，而且堆叠更多的层会带来更多的问题，如梯度爆炸、梯度消失、网络退化(degradation)等。梯度爆炸、消失问题可以通过BN层与参数初始化来解决；但网络退化问题则没有很好的办法去解决。退化是指随着网络深度的增加，网络层数饱和，导致优化困难。而且训练误差与测试误差随着网络加深反而更大了，这种误差更大并不是由过拟合引起的。 <br>
**Method：** 作者通过一种残差结构，将输入的特征经过恒等映射输出出去来解决网络退化问题。实验结果表明残差网络更容易优化，可以通过训练更深的网络来获得更高的准确性。 <br>
**Results：** 使用了残差结构块的模型深度达到了152层(深度为VGG的8倍，但仍有更低的复杂度)，并在ImageNet测试集上达到了3.57%的top-5错误率，取得了ILSVRC 2015分类任务的第一名。还在ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation的任务中均取得了第一名。<br>

![ImageNet图像识别挑战赛](/my_img/imagenet.png)

从resnet开始图像的分类任务已经做到比人还准确了，所以后面ImageNet图像分类比赛也没再举行了。

## 残差结构设计思路

#### 残差表示(Residual Representations)

- VLAD是一种残差向量的字典编码表示，而Fisher Vector是VLAD的概率表示方法。它们都用于图像检索和分类的浅层表示，对于它们来说编码残差矢量比编码原始向量更有效
- 在计算机图形学中通常使用多重网格方法(Multigrid method)求解微分方程(PDE)，但可以用依赖于残差向量的分层预处理来替代多重网格方法。有研究表明，使用残差向量更好优化、收敛速度更快

#### 跨层连接(Shortcut Connections)

- 训练多层感知机时，在输入与输出之间加一层线性层
- GoogLeNet中中间层连接辅助分类器来解决梯度爆炸、消失
- Inception与highway也都用到了跨层连接

## 残差结构

![resnetf2](/my_img/resnetf2.png)

x为输入，H(x)为是我们想提取到的特征也就是输出。假设可以通过多个非线性层来逼近这个输出 H(x) 的话，同理它也可以逼近 H(x)-x 。那与其让网络学习去近似 H(x) 不如让网络去近似一个残差函数 F(x)=H(x)-x 。虽然这两种方式都能逼近我们要的输出，但网络学习的难易程度不同。<br>

残差结构可以公式化为如下式子，F指拟合的残差映射。残差函数可以为全连接层或卷积层，层数可以随意设置(但只有一层时无明显效果)。
![F1](https://latex.codecogs.com/svg.image?y=F(x,\left\{W_i&space;\right\})&plus;x)
当F与x维度相同时，可以逐个元素相加(卷积层逐个通道在两个特征图上逐像素相加)；但当维度不同时，需要给x加上一个线性映射，使它的维度与F相同。
![F2](https://latex.codecogs.com/svg.image?y=F(x,\left\{W_i&space;\right\})&plus;W_sx)

*卷积层时用1×1卷积进行维度匹配，这样又加了一层ReLU所以增加了网络的非线性拟合性。注意最后F与x相加时是先相加再用ReLU函数激活。*

#### 残差结构的特点

- 网络退化问题表明多个非线性层难以使H(x)直接逼近x，但通过恒等映射可以将参数学习为0，只通过跨层连接来逼近x。当网络不需要太深的时候，恒等映射的层就多一些，需要深一些的时候就都进行参数学习。这就实现了可控制深度，解决了网络退化问题。
- 当残差结构接近恒等映射时，网络更容易发现接近恒等映射的扰动，而不是学习一个新的函数。
- 通过某些层的恒等映射实现了多个模型的集成，而且删除ResNet的部分节点对整个模型的影响很小。
- 因为有恒等映射所以反向传播时总会有梯度回传，缓解了梯度消失。

## Architecture

## 实验细节

## 参考阅读

- [ResNet论文笔记及代码剖析](/https://zhuanlan.zhihu.com/p/56961832)


