---
layout:     post
title:      图神经网络综述阅读笔记（慢慢写）
subtitle:   A Comprehensive Survey on Graph Neural Networks
date:       2021-11-20
author:     fennel
header-img: /my_img/resnest50.jpg
catalog: true
tags:
    - 论文笔记
    - 深度学习
---

## 前言

![GNNcomp1](/my_img/GNNcomp1.png)
*左：二维欧式数据下的卷积；右：图数据下的卷积*

曾经在机器学习的特征工程中主要依靠人工提取特征，而现在通过端到端的深度学习框架(CNN、RNN、自编码器)可以更好的进行特征提取。深度学习的快速发展得益于计算机算力的大幅度提升、大规模的训练数据以及可以有效的从欧式数据中提取潜在特征。虽然深度学习可以有效地应用于欧式数据，但越来越多的数据是以图的形式来表示，如：化学分子结构、论文的引文系统。图中有大小可变的无序节点，每个节点的邻居数目可能不同，导致卷积操作难以应用于图数据。且图中每个节点不再独立，节点之间通过各种类型的连接与其他节点相关。为了处理这种不规则的图数据，有了许多图数据深度学习方法的研究。现将这些图神经网络分为四类：
**图循环神经网络**、**图卷积神经网络**、**图自编码器**、**时空图神经网络**。<br>

*常用符号*
![GNNcomt1](/my_img/GNNcomt1.png)

---

## 研究背景

最早的图神经网络应用是1997年由Sperduti将神经网络应用于无环图，而图神经网络的概念在2005年由Gori提出。早期的研究都属于图循环神经网络，通过迭代传播邻居信息的方式来更新节点直至到达稳定，但计算量过大导致训练十分昂贵。后由于CNN在计算机视觉领域的成功，出现了大量图卷积的概念。图卷积神经网络主要分为两类，一类基于谱方法，一类基于空间方法。

#### GNN vs network embedding(网络嵌入表达)

network embedding旨在将网络中的节点表示为低维向量，同时保留网络拓扑结构信息与节点信息，以便使用现成工具即可轻松完成后续的图分析任务。其与GNN的主要区别在于GNN是一组为各种任务设计的神经网络模型，而network embedding涵盖了针对同一任务的各种方法，且network embedding中还包含了非深度学习的方法。

#### GNN vs graph kernel methods(图核方法)

graph kernel methods主要用于图分类问题，通过核函数来测量图之间的相似性，再通过支持向量机进行图监督学习。图核方法也是通过映射函数将图或节点压缩为向量，但不同于GNN图核方法的映射函数是确定的。

## 网络分类

- **图循环神经网络(RecGNNs)**：RecGNNs通过节点不断与其邻居交换信息来更新节点信息直至到达稳定状态，RecGNNs启发了后续的研究，特别是信息传播的思想。
- **图卷积神经网络(ConvGNNs)**：ConvGNNs将卷积操作从网格数据推广到图数据，主要思想为聚合自己的特征与邻居的特征来生成节点的表示。与RecGNNs不同ConvGNNs通过堆叠多个卷积层来获取节点的高级表示。
- **图自编码器(GAEs)**：GAEs是一种无监督学习，通过编码器-解码器架构将节点或图编码到潜在向量空间中，并根据编码信息重建图数据。GAEs主要用于学习network embedding和图生成。
- **时空图神经网络(STGNNs)**：STGNNS用于学习时空图中的信息，同时考虑空间与时间的依赖。当前许多方法使用图卷积捕获空间依赖并使用RNN或CNN捕获时间依赖。

![GNNcomp2](/my_img/GNNcomp2.png)
*ConvGNNs：具有多个图卷积层的ConvGNN，每堆叠一层卷积层，每个节点都能从更远的邻居接收消息。*
![GNNcomp3](/my_img/GNNcomp3.png)
*ConvGNNs：用于图分类的ConvGNN，通过池化层获取粗化图，再从粗化图上的节点学习到更高级的表示，最后通过Readout层获得图的低维表示用与分类。*
![GNNcomp4](/my_img/GNNcomp4.png)
*GAEs：编码器使用图卷积获取每个节点的embedding，解码器计算节点embedding之间的距离重建图邻接矩阵，通过最小化原邻接矩阵与重建邻接矩阵之间的差异来训练网络。*
![GNNcomp5](/my_img/GNNcomp5.png)
*STGNNs：在图卷积后加一个维CNN层，图卷积捕获空间依赖性，CNN沿时间轴滑动捕获时间依赖性。*

## 图循环神经网络RecGNNs

图循环神经网络是GNN的先驱，它通过反复应用相同的参数集来获取节点的表示。受限于计算效率早期研究主要集中在有向无环图。

![GNNcomp6](/my_img/GNNcomp6.png)
*RecGNNs使用相同的图循环层来更新节点*

- GNN：基于信息扩散机制，通过反复交换邻居信息来更新节点信息，直到达到稳定状态，每个节点的隐藏状态都被重复更新。用求和操作来聚合邻居信息使得GNN适用于所有节点，即使邻居数量不同且不知道邻居顺序。
- GraphESN：使用echo state网络以提高模型训练效率，GraphESN由编码器和输出层组成，实现了一个收缩状态转换函数来循环更新节点状态，通过将固定节点状态作为输入来训练输出层。
- GGNN：采用门控循环单元(GRU)，将循环减少到固定的步数。节点隐藏状态由其先前的隐藏状态及其相邻的隐藏状态更新，但GGNN需要在所有节点上多次运行循环函数，且需要将所有节点的隐藏状态存储在内存中。
- SSE：提出了一种对大图的学习算法，以随机和异步的方式循环更新节点隐藏状态，交替采样一些节点用于状态更新，一些节点进行梯度计算。SSE的循环函数定义为历史状态和新状态的加权平均值。
