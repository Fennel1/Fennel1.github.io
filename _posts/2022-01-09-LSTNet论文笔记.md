---
layout:     post
title:      LSTNet论文笔记
subtitle:   Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks
date:       2022-01-09
author:     fennel
header-img: /my_img/resnest50.jpg
catalog: true
tags:
    - 深度学习
---

## 论文pdf

- [Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks](/paper/Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks.pdf)

--- 

## 前言

**Purpose**：多元时间序列预测是一个重要的机器学习问题。而在许多实际应用中时序数据往往是长短期混合的，这就导致如自回归模型和高斯过程等传统方法失效。<br>
**Method**：提出了一种新颖的深度学习框架，即长期和短期时间序列网络 (LSTNet)。LSTNet 使用 CNN 和 RNN 来提取数据中的短期局部依赖模式，并发现时间序列趋势的长期模式。并且利用传统的自回归模型来解决神经网络模型的尺度不敏感问题<br>
**Results**：LSTNet 显着改善了多个基准数据集上时间序列预测的结果，成功地捕获了数据中的短期和长期重复模式，并结合了线性和非线性模型来进行预测。<br>

![LSTNetf1](my_img/LSTNetf1.png)
目前如何捕获多个变量之间的动态依赖是多元时序预测面临的问题，就是在现实需求往往需要短期与长期循环的混合模式。就比如上图是每小时高速公路车流量，对于它而言短期循环模式是每天(短期)与每周(长期)的循环。短期反应早高峰与晚高峰的时间；长期反应工作日和周末的区别。而一个成功的时序预测模型需要同时准确预测短期与长期的重复模式。传统的方法都无法有效区分长短期的循环模式。

*预测太阳能输出问题：
短期：每一时刻云层运动、风向变化
长期：白天与夜晚、夏季与冬季
*

---

## 设计思路

- **循环神经网络(RNN)**：在自然语言处理任务中有很不错的效果。且RNN的两种变体：长短期记忆(LSTM)与门控循环单元(GRU)在机器翻译、语音领域的任务中可以有效地捕获长期和短期的词义。
- **卷积神经网络(CNN)**：卷积神经网络具有平移不变性与局部性，基于这两种特性在计算机视觉领域有不错的性能。

通过CNN捕获输入的多元时序数据与循环层之间的局部依赖，以此来捕获长期的依赖关系，再通过RNN捕获短期的依赖。


## Architecture

![LSTNetf1](/my_img/LSTNetf2.png)

#### CNN部分

LSTNet的第一部分是没有pooling层的CNN，目的是捕获短期特征。

![](https://latex.codecogs.com/svg.image?h_k=RELU(W_k*X&plus;b_k))
![](https://latex.codecogs.com/svg.image?X=\{y_1,y_2,...,y_T\}\in&space;R^{n\times&space;T})
![](https://latex.codecogs.com/svg.image?W_k\in&space;R^{n\times&space;w})

输入X为输入矩阵，\*为卷积操作，所以输出的h为一个向量，设置多个卷积核就可以得到多个向量。

#### RNN部分

![](/my_img/LSTNetf3.png)
*在更新隐藏状态处作者使用了ReLU作为激活函数，因为使用ReLU激活函数在反向传播时梯度不易消失，更容易传播。*


在CNN后加入了门控循环单元并使用ReLU激活函数作为隐藏状态的激活函数，重置门与更新门则使用sigmoid作为激活函数。

#### Recurrent-skip部分

由于梯度消失GRU与LSTM不容易捕获长期的依赖，所以在GRU中加入Recurrent-skip。
所以GRU的迭代式子可以修改为：

作者使用dense layer将RNN与Recurrent-skip的输出结合在一起，接收的输入为RNN在t时刻的隐藏状态与Recurrent-skip在t时刻以前的所有隐藏状态。


#### 注意力机制

上面的Recurrent-skip有一个严重的问题就是跳跃间隔p需要预定义，所以引入注意力机制。
注意力分数计算：

注意力分数的计算函数为：点积、余弦或由简单的多层感知器。
最后注意力层第t层的输出为连接加权向量与t-1层的隐藏状态再做一个线性变换。


#### 自回归



