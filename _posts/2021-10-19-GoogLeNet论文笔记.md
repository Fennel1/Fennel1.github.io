---
layout:     post
title:      GoogLeNet论文笔记
subtitle:   Going deeper with convolutions
date:       2021-10-19
author:     fennel
header-img: /my_img/resnest50.jpg
catalog: true
tags:
    - 论文笔记
---

-- 论文pdf

- [Going deeper with convolutions](/paper/googlenet.pdf)

## 前言

GoogLeNet是一个基于Hebbian法则和多尺度处理构建的网络结构，提出了Inception结构块。GoogLeNet共22层，参数数量与ALexNet相比较减少了12倍，但效率有很明显的提升。以6.67%的top-5误差取得了2014年ImageNet挑战赛图像分类的第一名。<br>

随着移动设备与嵌入式计算的不断发展，算法的效率以及对内存的使用率变得越来越重要。而GoogLeNet在结构设计上考虑了这个因素，使得模型可以用于学术界与工业界。

Inception结构中的1×1卷积层从来自于NiN，加入1×1卷积层主要有两个目的：

- 作为特征降维模块，以减少计算量，使模型可以更深更大
- 在增加模型深度、宽度的同时不会显著降低性能

## Inception结构设计思路

提高深度神经网络性能最直接的方法是将模型设计的更深、更宽，可这个方案有两个缺点：

- 更大的模型就有更多的参数，就更容易发生过拟合；更大的模型需要更多的数据样本，而高质量的训练集往往很贵
- 要求更高的计算效率，而且更大的网络利用率未必高(会有很多参数输出接近于0)

解决这两个问题的方法是将全连接层与卷积层转换为稀疏连接的架构。但是在非均匀稀疏结构上进行计算的效率非常低，即使减少运算次数，也会在查找与缓存操作上花费大量计算量，以至于切换为系数矩阵回报很小。在关于稀疏矩阵计算的大量文献表明将稀疏矩阵聚类为相对密集的子矩阵往往会为稀疏矩阵乘法提供优异的性能，也是由此产生了Inception结构，经过测试在计算机视觉领域取得了成功。

## Architecture

![googlenetf2](/my_img/googlenetf2.png)

- Inception结构的主要思想就是找到密集结构组件去逼近和替换一个最优的局部稀疏结构，并且因为卷积的平移不变性，使我们可以在空间上不断重复这种密集结构

### 初始版本 (a)

Arora提出了一种逐层的结构，它对上一层进行相关统计分析，并将高相关性的单元聚类在一起组成下一层。假设下层的单元都与输入图像的某些区域相关，且这些单元都分布在滤波器组中，而在接近输入的层中相关单元会在一个区域中大量聚集。这样很多聚类关注的是同一个区域，在下一层中可以通过1×1卷积覆盖。在更高层聚类越来越大数量越来越少，所以可以通过更大的卷积来覆盖更大的聚类。为了方便对齐，仅使用1×1、3×3、5×5的卷积核。Inception将这些层的结果合并到一起，输出到下一层中，此外添加了池化层提升效果。<br>

关于合并结果：假设上一层输出为28×28×192，则：

|  | 1×1卷积核 | 3×3卷积核 | 5×5卷积核 | 3×3池化层 | 合并 |
|:---:| :---: | :---: | :---: | :----: | :---: |
| 卷积核 | 1×1×64 | 3×3×128 | 5×5×32 | 3×3 |  |
| padding | 0 | 1 | 2 | 1 | |
| stride | 1 | 1 | 1 | 1 | |
| 输出 | 28×28×64 | 28×28×128 | 28×28×32 | 28×28×192 | 28×28×416 |


### 改进版本 (b)
